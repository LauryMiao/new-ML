{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path: ~/ml/catdog\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "img = load_img('./train/cats/cat.0.jpg')  # this is a PIL image\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (374, 500, 3)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 374, 500, 3)\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1,\n",
    "                          save_to_dir='preview', save_prefix='cat', save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break  # otherwise the generator would loop indefinitely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 374, 500, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(374, 500, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = img_to_array(load_img('./train/cats/cat.0.jpg'))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n",
      "Found 5000 images belonging to 2 classes.\n",
      "Epoch 1/50\n",
      "125/125 [==============================] - 48s - loss: 0.7088 - acc: 0.5325 - val_loss: 0.6810 - val_acc: 0.6012\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 42s - loss: 0.6942 - acc: 0.5925 - val_loss: 0.6312 - val_acc: 0.6012\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 44s - loss: 0.6439 - acc: 0.6295 - val_loss: 0.6300 - val_acc: 0.6312\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 40s - loss: 0.6458 - acc: 0.6290 - val_loss: 0.5853 - val_acc: 0.6950\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 52s - loss: 0.6385 - acc: 0.6615 - val_loss: 0.6075 - val_acc: 0.6925\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 48s - loss: 0.6225 - acc: 0.6550 - val_loss: 0.5634 - val_acc: 0.7063\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 48s - loss: 0.6288 - acc: 0.6520 - val_loss: 0.5540 - val_acc: 0.7311\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 53s - loss: 0.6232 - acc: 0.6635 - val_loss: 0.5549 - val_acc: 0.7262\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 52s - loss: 0.5885 - acc: 0.7035 - val_loss: 0.5625 - val_acc: 0.7137\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 52s - loss: 0.5822 - acc: 0.6950 - val_loss: 0.5450 - val_acc: 0.7312\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 48s - loss: 0.5798 - acc: 0.6970 - val_loss: 0.6274 - val_acc: 0.6613\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 50s - loss: 0.5681 - acc: 0.7045 - val_loss: 0.4846 - val_acc: 0.7950\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 50s - loss: 0.5554 - acc: 0.7345 - val_loss: 0.5480 - val_acc: 0.7412\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 49s - loss: 0.5581 - acc: 0.7315 - val_loss: 0.4502 - val_acc: 0.7837\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 50s - loss: 0.5453 - acc: 0.7360 - val_loss: 0.5111 - val_acc: 0.7650\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 52s - loss: 0.5792 - acc: 0.7270 - val_loss: 0.5120 - val_acc: 0.7600\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 51s - loss: 0.5556 - acc: 0.7300 - val_loss: 0.4923 - val_acc: 0.7688\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 50s - loss: 0.5506 - acc: 0.7430 - val_loss: 0.4732 - val_acc: 0.7975\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 51s - loss: 0.5364 - acc: 0.7480 - val_loss: 0.4904 - val_acc: 0.7740\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 51s - loss: 0.5442 - acc: 0.7330 - val_loss: 0.4704 - val_acc: 0.7738\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 48s - loss: 0.5181 - acc: 0.7560 - val_loss: 0.4715 - val_acc: 0.7937\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 53s - loss: 0.5224 - acc: 0.7490 - val_loss: 0.5015 - val_acc: 0.7688\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 50s - loss: 0.5261 - acc: 0.7490 - val_loss: 0.4882 - val_acc: 0.7750\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 45s - loss: 0.5192 - acc: 0.7680 - val_loss: 0.5223 - val_acc: 0.7612\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 40s - loss: 0.5226 - acc: 0.7460 - val_loss: 0.4392 - val_acc: 0.8030\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 48s - loss: 0.5244 - acc: 0.7540 - val_loss: 0.4650 - val_acc: 0.7800\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 51s - loss: 0.4991 - acc: 0.7625 - val_loss: 0.4424 - val_acc: 0.7950\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 50s - loss: 0.5158 - acc: 0.7710 - val_loss: 0.4721 - val_acc: 0.7887\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 50s - loss: 0.5254 - acc: 0.7610 - val_loss: 0.4699 - val_acc: 0.7812\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 50s - loss: 0.5138 - acc: 0.7645 - val_loss: 0.4220 - val_acc: 0.8225\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 51s - loss: 0.5122 - acc: 0.7610 - val_loss: 0.4995 - val_acc: 0.7525\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 50s - loss: 0.5013 - acc: 0.7735 - val_loss: 0.4477 - val_acc: 0.8050\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 51s - loss: 0.5172 - acc: 0.7620 - val_loss: 0.4579 - val_acc: 0.8137\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 50s - loss: 0.5027 - acc: 0.7835 - val_loss: 0.4710 - val_acc: 0.7675\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 50s - loss: 0.4987 - acc: 0.7775 - val_loss: 0.4600 - val_acc: 0.7850\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 47s - loss: 0.5130 - acc: 0.7645 - val_loss: 0.4318 - val_acc: 0.8037\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 48s - loss: 0.4881 - acc: 0.7830 - val_loss: 0.3797 - val_acc: 0.8194\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 51s - loss: 0.5002 - acc: 0.7705 - val_loss: 0.4233 - val_acc: 0.8025\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 40s - loss: 0.5009 - acc: 0.7630 - val_loss: 0.4525 - val_acc: 0.7950\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 49s - loss: 0.4947 - acc: 0.7765 - val_loss: 0.4635 - val_acc: 0.7987\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 51s - loss: 0.4973 - acc: 0.7745 - val_loss: 0.4865 - val_acc: 0.7812\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 49s - loss: 0.5102 - acc: 0.7690 - val_loss: 0.4061 - val_acc: 0.8337\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 49s - loss: 0.4799 - acc: 0.7835 - val_loss: 0.4440 - val_acc: 0.7955\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 47s - loss: 0.4841 - acc: 0.7870 - val_loss: 0.4186 - val_acc: 0.8200\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 50s - loss: 0.4772 - acc: 0.7905 - val_loss: 0.4034 - val_acc: 0.7975\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 49s - loss: 0.4775 - acc: 0.7950 - val_loss: 0.4387 - val_acc: 0.8137\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 49s - loss: 0.4813 - acc: 0.7930 - val_loss: 0.4533 - val_acc: 0.8000\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 43s - loss: 0.5003 - acc: 0.7840 - val_loss: 0.4031 - val_acc: 0.8125\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 49s - loss: 0.4618 - acc: 0.7885 - val_loss: 0.4415 - val_acc: 0.8295\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 50s - loss: 0.4879 - acc: 0.7780 - val_loss: 0.4748 - val_acc: 0.7738\n"
     ]
    }
   ],
   "source": [
    "'''This script goes along the blog post\n",
    "\"Building powerful image classification models using very little data\"\n",
    "from blog.keras.io.\n",
    "It uses data that can be downloaded at:\n",
    "https://www.kaggle.com/c/dogs-vs-cats/data\n",
    "In our setup, we:\n",
    "- created a catdog/ folder\n",
    "- created train/ and validation/ subfolders inside data/\n",
    "- created cats/ and dogs/ subfolders inside train/ and validation/\n",
    "- put the cat pictures index 0-999 in data/train/cats\n",
    "- put the cat pictures index 1000-1400 in data/validation/cats\n",
    "- put the dogs pictures index 12500-13499 in data/train/dogs\n",
    "- put the dog pictures index 13500-13900 in data/validation/dogs\n",
    "So that we have 1000 training examples for each class, and 400 validation examples for each class.\n",
    "In summary, this is our directory structure:\n",
    "```\n",
    "catdog/\n",
    "    train/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "```\n",
    "'''\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = './train'\n",
    "validation_data_dir = './validation'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "model.save_weights('first_try.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 148, 148, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 72, 72, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 72, 72, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 34, 34, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                1183808   \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,212,513\n",
      "Trainable params: 1,212,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Using real-time data augmentation.\n",
      "Epoch 1/200\n",
      "1562/1562 [==============================] - 131s - loss: 1.8248 - acc: 0.3243 - val_loss: 1.5924 - val_acc: 0.4176\n",
      "Epoch 2/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.5486 - acc: 0.4327 - val_loss: 1.4077 - val_acc: 0.4896\n",
      "Epoch 3/200\n",
      "1562/1562 [==============================] - 132s - loss: 1.4285 - acc: 0.4825 - val_loss: 1.2547 - val_acc: 0.5503\n",
      "Epoch 4/200\n",
      "1562/1562 [==============================] - 144s - loss: 1.3384 - acc: 0.5216 - val_loss: 1.1590 - val_acc: 0.5906\n",
      "Epoch 5/200\n",
      "1562/1562 [==============================] - 123s - loss: 1.2692 - acc: 0.5488 - val_loss: 1.0915 - val_acc: 0.6097\n",
      "Epoch 6/200\n",
      "1562/1562 [==============================] - 124s - loss: 1.2101 - acc: 0.5727 - val_loss: 1.0253 - val_acc: 0.6409\n",
      "Epoch 7/200\n",
      "1562/1562 [==============================] - 132s - loss: 1.1507 - acc: 0.5924 - val_loss: 0.9876 - val_acc: 0.6570\n",
      "Epoch 8/200\n",
      "1562/1562 [==============================] - 136s - loss: 1.1086 - acc: 0.6077 - val_loss: 0.9389 - val_acc: 0.6728\n",
      "Epoch 9/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.0685 - acc: 0.6230 - val_loss: 0.9263 - val_acc: 0.6781\n",
      "Epoch 10/200\n",
      "1562/1562 [==============================] - 136s - loss: 1.0365 - acc: 0.6359 - val_loss: 0.8971 - val_acc: 0.6892\n",
      "Epoch 11/200\n",
      "1562/1562 [==============================] - 125s - loss: 1.0071 - acc: 0.6453 - val_loss: 0.8515 - val_acc: 0.7061\n",
      "Epoch 12/200\n",
      "1562/1562 [==============================] - 133s - loss: 0.9818 - acc: 0.6565 - val_loss: 0.8382 - val_acc: 0.7078\n",
      "Epoch 13/200\n",
      "1562/1562 [==============================] - 110s - loss: 0.9666 - acc: 0.6630 - val_loss: 0.8913 - val_acc: 0.6878\n",
      "Epoch 14/200\n",
      "1562/1562 [==============================] - 110s - loss: 0.9416 - acc: 0.6701 - val_loss: 0.8179 - val_acc: 0.7161\n",
      "Epoch 15/200\n",
      "1562/1562 [==============================] - 110s - loss: 0.9231 - acc: 0.6772 - val_loss: 0.7808 - val_acc: 0.7332\n",
      "Epoch 16/200\n",
      "1562/1562 [==============================] - 111s - loss: 0.9121 - acc: 0.6812 - val_loss: 0.7772 - val_acc: 0.7313\n",
      "Epoch 17/200\n",
      "1562/1562 [==============================] - 110s - loss: 0.8919 - acc: 0.6916 - val_loss: 0.7642 - val_acc: 0.7324\n",
      "Epoch 18/200\n",
      "1562/1562 [==============================] - 110s - loss: 0.8811 - acc: 0.6932 - val_loss: 0.7443 - val_acc: 0.7413\n",
      "Epoch 19/200\n",
      "1562/1562 [==============================] - 112s - loss: 0.8716 - acc: 0.6966 - val_loss: 0.7300 - val_acc: 0.7496\n",
      "Epoch 20/200\n",
      "1562/1562 [==============================] - 108s - loss: 0.8611 - acc: 0.7009 - val_loss: 0.7327 - val_acc: 0.7453\n",
      "Epoch 21/200\n",
      "1562/1562 [==============================] - 111s - loss: 0.8492 - acc: 0.7057 - val_loss: 0.7534 - val_acc: 0.7406\n",
      "Epoch 22/200\n",
      "1562/1562 [==============================] - 111s - loss: 0.8422 - acc: 0.7093 - val_loss: 0.7111 - val_acc: 0.7540\n",
      "Epoch 23/200\n",
      "1562/1562 [==============================] - 111s - loss: 0.8353 - acc: 0.7104 - val_loss: 0.7251 - val_acc: 0.7469\n",
      "Epoch 24/200\n",
      "1562/1562 [==============================] - 111s - loss: 0.8276 - acc: 0.7124 - val_loss: 0.7032 - val_acc: 0.7585\n",
      "Epoch 25/200\n",
      "1562/1562 [==============================] - 111s - loss: 0.8212 - acc: 0.7157 - val_loss: 0.6934 - val_acc: 0.7591\n",
      "Epoch 26/200\n",
      "1562/1562 [==============================] - 112s - loss: 0.8157 - acc: 0.7177 - val_loss: 0.7029 - val_acc: 0.7611\n",
      "Epoch 27/200\n",
      "1562/1562 [==============================] - 114s - loss: 0.8158 - acc: 0.7182 - val_loss: 0.6978 - val_acc: 0.75637\n",
      "Epoch 28/200\n",
      "1562/1562 [==============================] - 111s - loss: 0.8034 - acc: 0.7219 - val_loss: 0.6737 - val_acc: 0.7665\n",
      "Epoch 29/200\n",
      "1562/1562 [==============================] - 113s - loss: 0.8028 - acc: 0.7252 - val_loss: 0.6638 - val_acc: 0.7710\n",
      "Epoch 30/200\n",
      "1562/1562 [==============================] - 112s - loss: 0.7970 - acc: 0.7271 - val_loss: 0.6754 - val_acc: 0.7742\n",
      "Epoch 31/200\n",
      "1562/1562 [==============================] - 112s - loss: 0.7944 - acc: 0.7284 - val_loss: 0.6689 - val_acc: 0.7718\n",
      "Epoch 32/200\n",
      "1562/1562 [==============================] - 111s - loss: 0.7901 - acc: 0.7290 - val_loss: 0.6573 - val_acc: 0.7730\n",
      "Epoch 33/200\n",
      "1562/1562 [==============================] - 112s - loss: 0.7921 - acc: 0.7283 - val_loss: 0.6705 - val_acc: 0.7726\n",
      "Epoch 34/200\n",
      "1562/1562 [==============================] - 111s - loss: 0.7814 - acc: 0.7310 - val_loss: 0.6828 - val_acc: 0.7646\n",
      "Epoch 35/200\n",
      "1562/1562 [==============================] - 110s - loss: 0.7836 - acc: 0.7342 - val_loss: 0.6676 - val_acc: 0.7682\n",
      "Epoch 36/200\n",
      "1562/1562 [==============================] - 110s - loss: 0.7802 - acc: 0.7358 - val_loss: 0.6846 - val_acc: 0.7644\n",
      "Epoch 37/200\n",
      "1562/1562 [==============================] - 111s - loss: 0.7805 - acc: 0.7332 - val_loss: 0.6531 - val_acc: 0.7802\n",
      "Epoch 38/200\n",
      "1562/1562 [==============================] - 112s - loss: 0.7738 - acc: 0.7361 - val_loss: 0.6517 - val_acc: 0.7781\n",
      "Epoch 39/200\n",
      "1562/1562 [==============================] - 111s - loss: 0.7726 - acc: 0.7378 - val_loss: 0.6519 - val_acc: 0.7772\n",
      "Epoch 40/200\n",
      "1562/1562 [==============================] - 156s - loss: 0.7676 - acc: 0.7373 - val_loss: 0.6569 - val_acc: 0.7799\n",
      "Epoch 41/200\n",
      "1562/1562 [==============================] - 150s - loss: 0.7659 - acc: 0.7400 - val_loss: 0.6359 - val_acc: 0.7887\n",
      "Epoch 42/200\n",
      "1562/1562 [==============================] - 111s - loss: 0.7608 - acc: 0.7416 - val_loss: 0.7360 - val_acc: 0.7597\n",
      "Epoch 43/200\n",
      "1562/1562 [==============================] - 109s - loss: 0.7621 - acc: 0.7389 - val_loss: 0.6581 - val_acc: 0.7795\n",
      "Epoch 44/200\n",
      "1562/1562 [==============================] - 111s - loss: 0.7598 - acc: 0.7429 - val_loss: 0.6338 - val_acc: 0.7864\n",
      "Epoch 45/200\n",
      "1562/1562 [==============================] - 111s - loss: 0.7614 - acc: 0.7431 - val_loss: 0.6400 - val_acc: 0.7895\n",
      "Epoch 46/200\n",
      "1562/1562 [==============================] - 113s - loss: 0.7607 - acc: 0.7417 - val_loss: 0.6547 - val_acc: 0.7823\n",
      "Epoch 47/200\n",
      "1562/1562 [==============================] - 130s - loss: 0.7562 - acc: 0.7415 - val_loss: 0.6631 - val_acc: 0.7789\n",
      "Epoch 48/200\n",
      "1562/1562 [==============================] - 123s - loss: 0.7503 - acc: 0.7439 - val_loss: 0.6564 - val_acc: 0.7802\n",
      "Epoch 49/200\n",
      "1562/1562 [==============================] - 115s - loss: 0.7552 - acc: 0.7458 - val_loss: 0.6418 - val_acc: 0.7811\n",
      "Epoch 50/200\n",
      "1562/1562 [==============================] - 156s - loss: 0.7495 - acc: 0.7480 - val_loss: 0.6672 - val_acc: 0.7779\n",
      "Epoch 51/200\n",
      "1562/1562 [==============================] - 107s - loss: 0.7527 - acc: 0.7449 - val_loss: 0.6310 - val_acc: 0.7911\n",
      "Epoch 52/200\n",
      "1562/1562 [==============================] - 129s - loss: 0.7475 - acc: 0.7491 - val_loss: 0.6211 - val_acc: 0.7902\n",
      "Epoch 53/200\n",
      "1562/1562 [==============================] - 150s - loss: 0.7490 - acc: 0.7455 - val_loss: 0.6350 - val_acc: 0.7879\n",
      "Epoch 54/200\n",
      "1562/1562 [==============================] - 154s - loss: 0.7530 - acc: 0.7466 - val_loss: 0.6386 - val_acc: 0.7845\n",
      "Epoch 55/200\n",
      "1562/1562 [==============================] - 141s - loss: 0.7401 - acc: 0.7498 - val_loss: 0.6567 - val_acc: 0.7798\n",
      "Epoch 56/200\n",
      "1562/1562 [==============================] - 157s - loss: 0.7456 - acc: 0.7507 - val_loss: 0.6845 - val_acc: 0.7678\n",
      "Epoch 57/200\n",
      "1562/1562 [==============================] - 159s - loss: 0.7401 - acc: 0.7492 - val_loss: 0.6341 - val_acc: 0.7873\n",
      "Epoch 58/200\n",
      "1562/1562 [==============================] - 160s - loss: 0.7415 - acc: 0.7511 - val_loss: 0.6713 - val_acc: 0.7806\n",
      "Epoch 59/200\n",
      "1562/1562 [==============================] - 128s - loss: 0.7433 - acc: 0.7510 - val_loss: 0.6485 - val_acc: 0.7923\n",
      "Epoch 60/200\n",
      "1562/1562 [==============================] - 158s - loss: 0.7405 - acc: 0.7491 - val_loss: 0.6463 - val_acc: 0.7890\n",
      "Epoch 61/200\n",
      "1562/1562 [==============================] - 156s - loss: 0.7471 - acc: 0.7511 - val_loss: 0.6329 - val_acc: 0.7842\n",
      "Epoch 62/200\n",
      "1562/1562 [==============================] - 150s - loss: 0.7442 - acc: 0.7517 - val_loss: 0.6146 - val_acc: 0.7914\n",
      "Epoch 63/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 163s - loss: 0.7434 - acc: 0.7506 - val_loss: 0.6338 - val_acc: 0.7888\n",
      "Epoch 64/200\n",
      "1562/1562 [==============================] - 116s - loss: 0.7431 - acc: 0.7514 - val_loss: 0.6231 - val_acc: 0.7926\n",
      "Epoch 65/200\n",
      "1562/1562 [==============================] - 142s - loss: 0.7414 - acc: 0.7520 - val_loss: 0.5990 - val_acc: 0.7949\n",
      "Epoch 66/200\n",
      "1562/1562 [==============================] - 147s - loss: 0.7420 - acc: 0.7508 - val_loss: 0.6202 - val_acc: 0.7906\n",
      "Epoch 67/200\n",
      "1562/1562 [==============================] - 139s - loss: 0.7379 - acc: 0.7519 - val_loss: 0.6581 - val_acc: 0.7852\n",
      "Epoch 68/200\n",
      "1562/1562 [==============================] - 130s - loss: 0.7397 - acc: 0.7534 - val_loss: 0.6729 - val_acc: 0.7748\n",
      "Epoch 69/200\n",
      "1562/1562 [==============================] - 119s - loss: 0.7419 - acc: 0.7516 - val_loss: 0.6187 - val_acc: 0.7915\n",
      "Epoch 70/200\n",
      "1562/1562 [==============================] - 128s - loss: 0.7411 - acc: 0.7518 - val_loss: 0.6259 - val_acc: 0.7935\n",
      "Epoch 71/200\n",
      "1562/1562 [==============================] - 122s - loss: 0.7372 - acc: 0.7541 - val_loss: 0.6530 - val_acc: 0.7894\n",
      "Epoch 72/200\n",
      "1562/1562 [==============================] - 136s - loss: 0.7379 - acc: 0.7538 - val_loss: 0.6053 - val_acc: 0.7966\n",
      "Epoch 73/200\n",
      "1562/1562 [==============================] - 128s - loss: 0.7357 - acc: 0.7539 - val_loss: 0.6272 - val_acc: 0.7902753\n",
      "Epoch 74/200\n",
      "1562/1562 [==============================] - 128s - loss: 0.7429 - acc: 0.7519 - val_loss: 0.6607 - val_acc: 0.7793\n",
      "Epoch 75/200\n",
      "1562/1562 [==============================] - 125s - loss: 0.7420 - acc: 0.7511 - val_loss: 0.6660 - val_acc: 0.7837\n",
      "Epoch 76/200\n",
      "1562/1562 [==============================] - 130s - loss: 0.7456 - acc: 0.7507 - val_loss: 0.6283 - val_acc: 0.7887\n",
      "Epoch 77/200\n",
      "1562/1562 [==============================] - 129s - loss: 0.7343 - acc: 0.7560 - val_loss: 0.7271 - val_acc: 0.7568\n",
      "Epoch 78/200\n",
      "1562/1562 [==============================] - 133s - loss: 0.7469 - acc: 0.7506 - val_loss: 0.6473 - val_acc: 0.7858\n",
      "Epoch 79/200\n",
      "1562/1562 [==============================] - 113s - loss: 0.7418 - acc: 0.7523 - val_loss: 0.6134 - val_acc: 0.7970\n",
      "Epoch 80/200\n",
      "1562/1562 [==============================] - 146s - loss: 0.7446 - acc: 0.7510 - val_loss: 0.6395 - val_acc: 0.7874\n",
      "Epoch 81/200\n",
      "1562/1562 [==============================] - 136s - loss: 0.7381 - acc: 0.7548 - val_loss: 0.6528 - val_acc: 0.7833\n",
      "Epoch 82/200\n",
      "1562/1562 [==============================] - 129s - loss: 0.7457 - acc: 0.7511 - val_loss: 0.6837 - val_acc: 0.7757\n",
      "Epoch 83/200\n",
      "1562/1562 [==============================] - 130s - loss: 0.7518 - acc: 0.7512 - val_loss: 0.6323 - val_acc: 0.7931\n",
      "Epoch 84/200\n",
      "1562/1562 [==============================] - 122s - loss: 0.7426 - acc: 0.7520 - val_loss: 0.7092 - val_acc: 0.7730\n",
      "Epoch 85/200\n",
      "1562/1562 [==============================] - 133s - loss: 0.7440 - acc: 0.7517 - val_loss: 0.6625 - val_acc: 0.7800\n",
      "Epoch 86/200\n",
      "1562/1562 [==============================] - 119s - loss: 0.7448 - acc: 0.7524 - val_loss: 0.6750 - val_acc: 0.7801\n",
      "Epoch 87/200\n",
      "1562/1562 [==============================] - 127s - loss: 0.7544 - acc: 0.7486 - val_loss: 0.6593 - val_acc: 0.7773\n",
      "Epoch 88/200\n",
      "1562/1562 [==============================] - 137s - loss: 0.7490 - acc: 0.7532 - val_loss: 0.6752 - val_acc: 0.7845\n",
      "Epoch 89/200\n",
      "1562/1562 [==============================] - 136s - loss: 0.7513 - acc: 0.7492 - val_loss: 0.6802 - val_acc: 0.7809\n",
      "Epoch 90/200\n",
      "1562/1562 [==============================] - 124s - loss: 0.7465 - acc: 0.7518 - val_loss: 0.6244 - val_acc: 0.7958\n",
      "Epoch 91/200\n",
      "1562/1562 [==============================] - 135s - loss: 0.7504 - acc: 0.7501 - val_loss: 0.6685 - val_acc: 0.7787\n",
      "Epoch 92/200\n",
      "1562/1562 [==============================] - 132s - loss: 0.7460 - acc: 0.7539 - val_loss: 0.6422 - val_acc: 0.7848\n",
      "Epoch 93/200\n",
      "1562/1562 [==============================] - 128s - loss: 0.7556 - acc: 0.7515 - val_loss: 0.6746 - val_acc: 0.7783\n",
      "Epoch 94/200\n",
      "1562/1562 [==============================] - 127s - loss: 0.7505 - acc: 0.7504 - val_loss: 0.6524 - val_acc: 0.7845\n",
      "Epoch 95/200\n",
      "1562/1562 [==============================] - 119s - loss: 0.7513 - acc: 0.7507 - val_loss: 0.6902 - val_acc: 0.7770\n",
      "Epoch 96/200\n",
      "1562/1562 [==============================] - 123s - loss: 0.7562 - acc: 0.7510 - val_loss: 0.6196 - val_acc: 0.7945\n",
      "Epoch 97/200\n",
      "1562/1562 [==============================] - 134s - loss: 0.7596 - acc: 0.7482 - val_loss: 0.6416 - val_acc: 0.7891\n",
      "Epoch 98/200\n",
      "1562/1562 [==============================] - 132s - loss: 0.7597 - acc: 0.7488 - val_loss: 0.6993 - val_acc: 0.7684\n",
      "Epoch 99/200\n",
      "1562/1562 [==============================] - 143s - loss: 0.7568 - acc: 0.7496 - val_loss: 0.7401 - val_acc: 0.7447\n",
      "Epoch 100/200\n",
      "1562/1562 [==============================] - 141s - loss: 0.7593 - acc: 0.7481 - val_loss: 0.6612 - val_acc: 0.7802\n",
      "Epoch 101/200\n",
      "1562/1562 [==============================] - 119s - loss: 0.7606 - acc: 0.7480 - val_loss: 0.6939 - val_acc: 0.7813\n",
      "Epoch 102/200\n",
      "1562/1562 [==============================] - 127s - loss: 0.7717 - acc: 0.7437 - val_loss: 0.6940 - val_acc: 0.7728\n",
      "Epoch 103/200\n",
      "1562/1562 [==============================] - 113s - loss: 0.7657 - acc: 0.7473 - val_loss: 0.7253 - val_acc: 0.7655\n",
      "Epoch 104/200\n",
      "1562/1562 [==============================] - 120s - loss: 0.7577 - acc: 0.7494 - val_loss: 0.7103 - val_acc: 0.7721\n",
      "Epoch 105/200\n",
      "1562/1562 [==============================] - 138s - loss: 0.7709 - acc: 0.7472 - val_loss: 0.6489 - val_acc: 0.7914\n",
      "Epoch 106/200\n",
      "1562/1562 [==============================] - 134s - loss: 0.7665 - acc: 0.7481 - val_loss: 0.7003 - val_acc: 0.7644\n",
      "Epoch 107/200\n",
      "1562/1562 [==============================] - 134s - loss: 0.7664 - acc: 0.7472 - val_loss: 0.7392 - val_acc: 0.7681\n",
      "Epoch 108/200\n",
      "1562/1562 [==============================] - 127s - loss: 0.7670 - acc: 0.7466 - val_loss: 0.6247 - val_acc: 0.7886\n",
      "Epoch 109/200\n",
      "1562/1562 [==============================] - 143s - loss: 0.7806 - acc: 0.7424 - val_loss: 0.6278 - val_acc: 0.7907\n",
      "Epoch 110/200\n",
      "1562/1562 [==============================] - 127s - loss: 0.7651 - acc: 0.7481 - val_loss: 0.6888 - val_acc: 0.7801\n",
      "Epoch 111/200\n",
      "1562/1562 [==============================] - 128s - loss: 0.7785 - acc: 0.7439 - val_loss: 0.6924 - val_acc: 0.7678\n",
      "Epoch 112/200\n",
      "1562/1562 [==============================] - 122s - loss: 0.7787 - acc: 0.7438 - val_loss: 0.6449 - val_acc: 0.7832\n",
      "Epoch 113/200\n",
      "1562/1562 [==============================] - 120s - loss: 0.7877 - acc: 0.7433 - val_loss: 0.7465 - val_acc: 0.7587\n",
      "Epoch 114/200\n",
      "1562/1562 [==============================] - 141s - loss: 0.7813 - acc: 0.7430 - val_loss: 0.6867 - val_acc: 0.7721\n",
      "Epoch 115/200\n",
      "1562/1562 [==============================] - 141s - loss: 0.7811 - acc: 0.7414 - val_loss: 0.6336 - val_acc: 0.7868\n",
      "Epoch 116/200\n",
      "1562/1562 [==============================] - 124s - loss: 0.7879 - acc: 0.7436 - val_loss: 0.6308 - val_acc: 0.7888\n",
      "Epoch 117/200\n",
      "1562/1562 [==============================] - 125s - loss: 0.7859 - acc: 0.7451 - val_loss: 0.6600 - val_acc: 0.7803\n",
      "Epoch 118/200\n",
      "1562/1562 [==============================] - 129s - loss: 0.7840 - acc: 0.7425 - val_loss: 0.6716 - val_acc: 0.7720\n",
      "Epoch 119/200\n",
      "1562/1562 [==============================] - 131s - loss: 0.7841 - acc: 0.7427 - val_loss: 0.7205 - val_acc: 0.7843\n",
      "Epoch 120/200\n",
      "1562/1562 [==============================] - 126s - loss: 0.7901 - acc: 0.7397 - val_loss: 0.7327 - val_acc: 0.7735\n",
      "Epoch 121/200\n",
      "1562/1562 [==============================] - 125s - loss: 0.7992 - acc: 0.7370 - val_loss: 0.6683 - val_acc: 0.7809\n",
      "Epoch 122/200\n",
      "1562/1562 [==============================] - 135s - loss: 0.7972 - acc: 0.7388 - val_loss: 0.6510 - val_acc: 0.7901\n",
      "Epoch 123/200\n",
      "1562/1562 [==============================] - 123s - loss: 0.8017 - acc: 0.7378 - val_loss: 0.6744 - val_acc: 0.7768\n",
      "Epoch 124/200\n",
      "1562/1562 [==============================] - 135s - loss: 0.7991 - acc: 0.7391 - val_loss: 0.6866 - val_acc: 0.7770\n",
      "Epoch 125/200\n",
      "1562/1562 [==============================] - 129s - loss: 0.8024 - acc: 0.7363 - val_loss: 0.6696 - val_acc: 0.7784\n",
      "Epoch 126/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 141s - loss: 0.8019 - acc: 0.7369 - val_loss: 0.7112 - val_acc: 0.7631\n",
      "Epoch 127/200\n",
      "1562/1562 [==============================] - 134s - loss: 0.8007 - acc: 0.7380 - val_loss: 0.6932 - val_acc: 0.7673\n",
      "Epoch 128/200\n",
      "1562/1562 [==============================] - 125s - loss: 0.8062 - acc: 0.7379 - val_loss: 0.7745 - val_acc: 0.7366\n",
      "Epoch 129/200\n",
      "1562/1562 [==============================] - 135s - loss: 0.8081 - acc: 0.7347 - val_loss: 0.8226 - val_acc: 0.7346\n",
      "Epoch 130/200\n",
      "1562/1562 [==============================] - 130s - loss: 0.8147 - acc: 0.7338 - val_loss: 0.7076 - val_acc: 0.7603\n",
      "Epoch 131/200\n",
      "1562/1562 [==============================] - 133s - loss: 0.8209 - acc: 0.7319 - val_loss: 0.6917 - val_acc: 0.7783\n",
      "Epoch 132/200\n",
      "1562/1562 [==============================] - 129s - loss: 0.8146 - acc: 0.7347 - val_loss: 0.6947 - val_acc: 0.7815\n",
      "Epoch 133/200\n",
      "1562/1562 [==============================] - 134s - loss: 0.8187 - acc: 0.7342 - val_loss: 0.7265 - val_acc: 0.7534\n",
      "Epoch 134/200\n",
      "1562/1562 [==============================] - 143s - loss: 0.8282 - acc: 0.7305 - val_loss: 0.6945 - val_acc: 0.7809\n",
      "Epoch 135/200\n",
      "1562/1562 [==============================] - 134s - loss: 0.8214 - acc: 0.7347 - val_loss: 0.8033 - val_acc: 0.7319\n",
      "Epoch 136/200\n",
      "1562/1562 [==============================] - 131s - loss: 0.8289 - acc: 0.7308 - val_loss: 0.7630 - val_acc: 0.7561\n",
      "Epoch 137/200\n",
      "1562/1562 [==============================] - 124s - loss: 0.8266 - acc: 0.7328 - val_loss: 0.7735 - val_acc: 0.7520\n",
      "Epoch 138/200\n",
      "1562/1562 [==============================] - 129s - loss: 0.8408 - acc: 0.7257 - val_loss: 0.7627 - val_acc: 0.7557\n",
      "Epoch 139/200\n",
      "1562/1562 [==============================] - 123s - loss: 0.8341 - acc: 0.7299 - val_loss: 0.6784 - val_acc: 0.7859\n",
      "Epoch 140/200\n",
      "1562/1562 [==============================] - 125s - loss: 0.8443 - acc: 0.7251 - val_loss: 0.7291 - val_acc: 0.7737\n",
      "Epoch 141/200\n",
      "1562/1562 [==============================] - 132s - loss: 0.8459 - acc: 0.7254 - val_loss: 0.7189 - val_acc: 0.7607\n",
      "Epoch 142/200\n",
      "1562/1562 [==============================] - 144s - loss: 0.8439 - acc: 0.7267 - val_loss: 0.7188 - val_acc: 0.7667\n",
      "Epoch 143/200\n",
      "1562/1562 [==============================] - 136s - loss: 0.8478 - acc: 0.7266 - val_loss: 0.6555 - val_acc: 0.7869\n",
      "Epoch 144/200\n",
      "1562/1562 [==============================] - 134s - loss: 0.8552 - acc: 0.7237 - val_loss: 0.7635 - val_acc: 0.7638\n",
      "Epoch 145/200\n",
      "1562/1562 [==============================] - 142s - loss: 0.8518 - acc: 0.7231 - val_loss: 0.7511 - val_acc: 0.7574\n",
      "Epoch 146/200\n",
      "1562/1562 [==============================] - 151s - loss: 0.8725 - acc: 0.7162 - val_loss: 0.7044 - val_acc: 0.7711\n",
      "Epoch 147/200\n",
      "1562/1562 [==============================] - 129s - loss: 0.8626 - acc: 0.7218 - val_loss: 0.7877 - val_acc: 0.7381\n",
      "Epoch 148/200\n",
      "1562/1562 [==============================] - 134s - loss: 0.8705 - acc: 0.7188 - val_loss: 0.6780 - val_acc: 0.7830\n",
      "Epoch 149/200\n",
      "1562/1562 [==============================] - 128s - loss: 0.8676 - acc: 0.7206 - val_loss: 0.7220 - val_acc: 0.7726\n",
      "Epoch 150/200\n",
      "1562/1562 [==============================] - 137s - loss: 0.8801 - acc: 0.7147 - val_loss: 0.8170 - val_acc: 0.7453\n",
      "Epoch 151/200\n",
      "1562/1562 [==============================] - 132s - loss: 0.8865 - acc: 0.7138 - val_loss: 0.7439 - val_acc: 0.7570\n",
      "Epoch 152/200\n",
      "1562/1562 [==============================] - 140s - loss: 0.8814 - acc: 0.7161 - val_loss: 0.7557 - val_acc: 0.7531\n",
      "Epoch 153/200\n",
      "1562/1562 [==============================] - 135s - loss: 0.9055 - acc: 0.7093 - val_loss: 0.7320 - val_acc: 0.7557\n",
      "Epoch 154/200\n",
      "1562/1562 [==============================] - 139s - loss: 0.8994 - acc: 0.7107 - val_loss: 0.8252 - val_acc: 0.7373\n",
      "Epoch 155/200\n",
      "1562/1562 [==============================] - 139s - loss: 0.9039 - acc: 0.7104 - val_loss: 0.7793 - val_acc: 0.7497\n",
      "Epoch 156/200\n",
      "1562/1562 [==============================] - 125s - loss: 0.9139 - acc: 0.7075 - val_loss: 0.7710 - val_acc: 0.7538\n",
      "Epoch 157/200\n",
      "1562/1562 [==============================] - 129s - loss: 0.9143 - acc: 0.7057 - val_loss: 0.7831 - val_acc: 0.7419\n",
      "Epoch 158/200\n",
      "1562/1562 [==============================] - 131s - loss: 0.9187 - acc: 0.7047 - val_loss: 0.8416 - val_acc: 0.7299\n",
      "Epoch 159/200\n",
      "1562/1562 [==============================] - 119s - loss: 0.9310 - acc: 0.7022 - val_loss: 0.9259 - val_acc: 0.7370\n",
      "Epoch 160/200\n",
      "1562/1562 [==============================] - 121s - loss: 0.9381 - acc: 0.6975 - val_loss: 0.8378 - val_acc: 0.7202\n",
      "Epoch 161/200\n",
      "1562/1562 [==============================] - 137s - loss: 0.9551 - acc: 0.6953 - val_loss: 1.0252 - val_acc: 0.6857\n",
      "Epoch 162/200\n",
      "1562/1562 [==============================] - 134s - loss: 0.9516 - acc: 0.6946 - val_loss: 0.7912 - val_acc: 0.7713\n",
      "Epoch 163/200\n",
      "1562/1562 [==============================] - 132s - loss: 0.9599 - acc: 0.6938 - val_loss: 0.8737 - val_acc: 0.7469\n",
      "Epoch 164/200\n",
      "1562/1562 [==============================] - 131s - loss: 0.9660 - acc: 0.6920 - val_loss: 0.7937 - val_acc: 0.7512\n",
      "Epoch 165/200\n",
      "1562/1562 [==============================] - 134s - loss: 0.9741 - acc: 0.6879 - val_loss: 0.9609 - val_acc: 0.7050\n",
      "Epoch 166/200\n",
      "1562/1562 [==============================] - 129s - loss: 0.9724 - acc: 0.6897 - val_loss: 1.0072 - val_acc: 0.6990\n",
      "Epoch 167/200\n",
      "1562/1562 [==============================] - 124s - loss: 0.9912 - acc: 0.6827 - val_loss: 0.9489 - val_acc: 0.6945\n",
      "Epoch 168/200\n",
      "1562/1562 [==============================] - 142s - loss: 1.0098 - acc: 0.6778 - val_loss: 1.0902 - val_acc: 0.6479\n",
      "Epoch 169/200\n",
      "1562/1562 [==============================] - 139s - loss: 1.0078 - acc: 0.6774 - val_loss: 0.8832 - val_acc: 0.7218\n",
      "Epoch 170/200\n",
      "1562/1562 [==============================] - 124s - loss: 1.0135 - acc: 0.6761 - val_loss: 1.1248 - val_acc: 0.6859\n",
      "Epoch 171/200\n",
      "1562/1562 [==============================] - 123s - loss: 1.0232 - acc: 0.6735 - val_loss: 0.9489 - val_acc: 0.6998\n",
      "Epoch 172/200\n",
      "1562/1562 [==============================] - 126s - loss: 1.0303 - acc: 0.6719 - val_loss: 1.0151 - val_acc: 0.6651\n",
      "Epoch 173/200\n",
      "1562/1562 [==============================] - 136s - loss: 1.0336 - acc: 0.6677 - val_loss: 0.9769 - val_acc: 0.6905\n",
      "Epoch 174/200\n",
      "1562/1562 [==============================] - 129s - loss: 1.0310 - acc: 0.6734 - val_loss: 0.9910 - val_acc: 0.7024\n",
      "Epoch 175/200\n",
      "1562/1562 [==============================] - 128s - loss: 1.0446 - acc: 0.6684 - val_loss: 1.0361 - val_acc: 0.6790\n",
      "Epoch 176/200\n",
      "1562/1562 [==============================] - 135s - loss: 1.0511 - acc: 0.6639 - val_loss: 0.8349 - val_acc: 0.7280\n",
      "Epoch 177/200\n",
      "1562/1562 [==============================] - 132s - loss: 1.0724 - acc: 0.6605 - val_loss: 1.0549 - val_acc: 0.6729\n",
      "Epoch 178/200\n",
      "1562/1562 [==============================] - 136s - loss: 1.0632 - acc: 0.6628 - val_loss: 0.8688 - val_acc: 0.7232\n",
      "Epoch 179/200\n",
      "1562/1562 [==============================] - 144s - loss: 1.0827 - acc: 0.6574 - val_loss: 1.1142 - val_acc: 0.6278\n",
      "Epoch 180/200\n",
      "1562/1562 [==============================] - 139s - loss: 1.0896 - acc: 0.6552 - val_loss: 1.2476 - val_acc: 0.5422\n",
      "Epoch 181/200\n",
      "1562/1562 [==============================] - 119s - loss: 1.0814 - acc: 0.6549 - val_loss: 1.0448 - val_acc: 0.6466\n",
      "Epoch 182/200\n",
      "1562/1562 [==============================] - 139s - loss: 1.1026 - acc: 0.6512 - val_loss: 0.9357 - val_acc: 0.7223\n",
      "Epoch 183/200\n",
      "1562/1562 [==============================] - 117s - loss: 1.1011 - acc: 0.6493 - val_loss: 0.9977 - val_acc: 0.7094\n",
      "Epoch 184/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.1008 - acc: 0.6537 - val_loss: 0.9981 - val_acc: 0.6842\n",
      "Epoch 185/200\n",
      "1562/1562 [==============================] - 128s - loss: 1.1147 - acc: 0.6466 - val_loss: 1.1191 - val_acc: 0.6469\n",
      "Epoch 186/200\n",
      "1562/1562 [==============================] - 143s - loss: 1.1123 - acc: 0.6437 - val_loss: 1.0988 - val_acc: 0.7038\n",
      "Epoch 187/200\n",
      "1562/1562 [==============================] - 135s - loss: 1.1289 - acc: 0.6419 - val_loss: 1.0306 - val_acc: 0.6976\n",
      "Epoch 188/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 128s - loss: 1.1296 - acc: 0.6426 - val_loss: 1.1455 - val_acc: 0.6232\n",
      "Epoch 189/200\n",
      "1562/1562 [==============================] - 132s - loss: 1.1396 - acc: 0.6378 - val_loss: 1.1179 - val_acc: 0.6249\n",
      "Epoch 190/200\n",
      "1562/1562 [==============================] - 126s - loss: 1.1343 - acc: 0.6387 - val_loss: 1.0670 - val_acc: 0.6938\n",
      "Epoch 191/200\n",
      "1562/1562 [==============================] - 131s - loss: 1.1553 - acc: 0.6325 - val_loss: 1.0719 - val_acc: 0.6253\n",
      "Epoch 192/200\n",
      "1562/1562 [==============================] - 131s - loss: 1.1679 - acc: 0.6279 - val_loss: 1.1734 - val_acc: 0.5959\n",
      "Epoch 193/200\n",
      "1562/1562 [==============================] - 141s - loss: 1.1657 - acc: 0.6327 - val_loss: 1.1382 - val_acc: 0.6252\n",
      "Epoch 194/200\n",
      "1562/1562 [==============================] - 128s - loss: 1.1883 - acc: 0.6232 - val_loss: 1.1176 - val_acc: 0.6717\n",
      "Epoch 195/200\n",
      "1562/1562 [==============================] - 118s - loss: 1.1900 - acc: 0.6221 - val_loss: 1.1957 - val_acc: 0.6027\n",
      "Epoch 196/200\n",
      "1562/1562 [==============================] - 143s - loss: 1.2185 - acc: 0.6148 - val_loss: 1.1633 - val_acc: 0.6318\n",
      "Epoch 197/200\n",
      "1562/1562 [==============================] - 138s - loss: 1.2253 - acc: 0.6106 - val_loss: 1.2743 - val_acc: 0.5875\n",
      "Epoch 198/200\n",
      "1562/1562 [==============================] - 131s - loss: 1.2326 - acc: 0.6075 - val_loss: 1.3930 - val_acc: 0.5631\n",
      "Epoch 199/200\n",
      "1562/1562 [==============================] - 133s - loss: 1.2433 - acc: 0.6048 - val_loss: 1.0770 - val_acc: 0.6536\n",
      "Epoch 200/200\n",
      "1562/1562 [==============================] - 132s - loss: 1.2543 - acc: 0.6009 - val_loss: 1.0727 - val_acc: 0.6695\n",
      "Saved trained model at /root/ml/catdog/saved_models/keras_cifar10_trained_model.h5 \n",
      " 9248/10000 [==========================>...] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "\n",
    "'''Train a simple deep CNN on the CIFAR10 small images dataset.\n",
    "GPU run command with Theano backend (with TensorFlow, the GPU is automatically used):\n",
    "    THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatx=float32 python cifar10_cnn.py\n",
    "It gets down to 0.65 test logloss in 25 epochs, and down to 0.55 after 50 epochs.\n",
    "(it's still underfitting at that point, though).\n",
    "'''\n",
    "\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "import os\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 200\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
