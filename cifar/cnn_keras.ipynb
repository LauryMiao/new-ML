{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Epoch 1/75\n",
      "781/781 [==============================] - 187s - loss: 1.8964 - acc: 0.4423 - val_loss: 1.1271 - val_acc: 0.6284\n",
      "Epoch 2/75\n",
      "781/781 [==============================] - 183s - loss: 1.4142 - acc: 0.5823 - val_loss: 1.0814 - val_acc: 0.6758\n",
      "Epoch 3/75\n",
      "781/781 [==============================] - 182s - loss: 1.3459 - acc: 0.6212 - val_loss: 1.3253 - val_acc: 0.6661\n",
      "Epoch 4/75\n",
      "781/781 [==============================] - 183s - loss: 1.2725 - acc: 0.6510 - val_loss: 1.1576 - val_acc: 0.6889\n",
      "Epoch 5/75\n",
      "781/781 [==============================] - 183s - loss: 1.1687 - acc: 0.6794 - val_loss: 0.8362 - val_acc: 0.7565\n",
      "Epoch 6/75\n",
      "781/781 [==============================] - 181s - loss: 1.1144 - acc: 0.6952 - val_loss: 0.8445 - val_acc: 0.7719\n",
      "Epoch 7/75\n",
      "781/781 [==============================] - 182s - loss: 1.0760 - acc: 0.7099 - val_loss: 0.8575 - val_acc: 0.7653\n",
      "Epoch 8/75\n",
      "781/781 [==============================] - 187s - loss: 1.0868 - acc: 0.7158 - val_loss: 0.7519 - val_acc: 0.7826\n",
      "Epoch 9/75\n",
      "781/781 [==============================] - 187s - loss: 1.1032 - acc: 0.7203 - val_loss: 0.9670 - val_acc: 0.7651\n",
      "Epoch 10/75\n",
      "781/781 [==============================] - 188s - loss: 1.1415 - acc: 0.7213 - val_loss: 3.3674 - val_acc: 0.6431\n",
      "Epoch 11/75\n",
      "781/781 [==============================] - 181s - loss: 1.1101 - acc: 0.7285 - val_loss: 0.7760 - val_acc: 0.7881\n",
      "Epoch 12/75\n",
      "781/781 [==============================] - 183s - loss: 1.0987 - acc: 0.7333 - val_loss: 0.7960 - val_acc: 0.7863\n",
      "Epoch 13/75\n",
      "781/781 [==============================] - 180s - loss: 1.0782 - acc: 0.7336 - val_loss: 0.7397 - val_acc: 0.8019\n",
      "Epoch 14/75\n",
      "781/781 [==============================] - 186s - loss: 1.0152 - acc: 0.7439 - val_loss: 0.7231 - val_acc: 0.8087\n",
      "Epoch 15/75\n",
      "781/781 [==============================] - 184s - loss: 0.9865 - acc: 0.7541 - val_loss: 0.7009 - val_acc: 0.8153\n",
      "Epoch 16/75\n",
      "781/781 [==============================] - 180s - loss: 0.9719 - acc: 0.7567 - val_loss: 0.7589 - val_acc: 0.8081\n",
      "Epoch 17/75\n",
      "781/781 [==============================] - 186s - loss: 0.9539 - acc: 0.7585 - val_loss: 0.7144 - val_acc: 0.8103\n",
      "Epoch 18/75\n",
      "781/781 [==============================] - 188s - loss: 0.9351 - acc: 0.7651 - val_loss: 0.7138 - val_acc: 0.8177\n",
      "Epoch 19/75\n",
      "781/781 [==============================] - 191s - loss: 0.9359 - acc: 0.7682 - val_loss: 0.8032 - val_acc: 0.8120\n",
      "Epoch 20/75\n",
      "781/781 [==============================] - 190s - loss: 0.9161 - acc: 0.7762 - val_loss: 0.6664 - val_acc: 0.8218\n",
      "Epoch 21/75\n",
      "781/781 [==============================] - 189s - loss: 0.9553 - acc: 0.7720 - val_loss: 0.7297 - val_acc: 0.8117\n",
      "Epoch 22/75\n",
      "781/781 [==============================] - 193s - loss: 0.9581 - acc: 0.7719 - val_loss: 0.7372 - val_acc: 0.8173\n",
      "Epoch 23/75\n",
      "781/781 [==============================] - 185s - loss: 0.9604 - acc: 0.7712 - val_loss: 0.6963 - val_acc: 0.8232\n",
      "Epoch 24/75\n",
      "781/781 [==============================] - 192s - loss: 0.9438 - acc: 0.7745 - val_loss: 0.7437 - val_acc: 0.8195\n",
      "Epoch 25/75\n",
      "781/781 [==============================] - 187s - loss: 0.9041 - acc: 0.7794 - val_loss: 0.6573 - val_acc: 0.8399\n",
      "Epoch 26/75\n",
      "781/781 [==============================] - 190s - loss: 0.8881 - acc: 0.7761 - val_loss: 0.6523 - val_acc: 0.8383\n",
      "Epoch 27/75\n",
      "781/781 [==============================] - 187s - loss: 0.8762 - acc: 0.7843 - val_loss: 0.6391 - val_acc: 0.8385\n",
      "Epoch 28/75\n",
      "781/781 [==============================] - 195s - loss: 0.8615 - acc: 0.7823 - val_loss: 0.6582 - val_acc: 0.8365\n",
      "Epoch 29/75\n",
      "781/781 [==============================] - 193s - loss: 0.8533 - acc: 0.7881 - val_loss: 0.6324 - val_acc: 0.8423\n",
      "Epoch 30/75\n",
      "781/781 [==============================] - 186s - loss: 0.8394 - acc: 0.7913 - val_loss: 0.6658 - val_acc: 0.8321\n",
      "Epoch 31/75\n",
      "781/781 [==============================] - 189s - loss: 0.8140 - acc: 0.7930 - val_loss: 0.6167 - val_acc: 0.8476\n",
      "Epoch 32/75\n",
      "781/781 [==============================] - 191s - loss: 0.8113 - acc: 0.7925 - val_loss: 0.6454 - val_acc: 0.8387\n",
      "Epoch 33/75\n",
      "781/781 [==============================] - 186s - loss: 0.7948 - acc: 0.7953 - val_loss: 0.6362 - val_acc: 0.8395\n",
      "Epoch 34/75\n",
      "781/781 [==============================] - 190s - loss: 0.7954 - acc: 0.7958 - val_loss: 0.6510 - val_acc: 0.8296\n",
      "Epoch 35/75\n",
      "781/781 [==============================] - 188s - loss: 0.7772 - acc: 0.7977 - val_loss: 0.5932 - val_acc: 0.8520\n",
      "Epoch 36/75\n",
      "781/781 [==============================] - 189s - loss: 0.7676 - acc: 0.7986 - val_loss: 0.6448 - val_acc: 0.8317\n",
      "Epoch 37/75\n",
      "781/781 [==============================] - 192s - loss: 0.7435 - acc: 0.8044 - val_loss: 0.6052 - val_acc: 0.8468\n",
      "Epoch 38/75\n",
      "781/781 [==============================] - 194s - loss: 0.7413 - acc: 0.8044 - val_loss: 0.6174 - val_acc: 0.8476\n",
      "Epoch 39/75\n",
      "781/781 [==============================] - 189s - loss: 0.7356 - acc: 0.8068 - val_loss: 0.6164 - val_acc: 0.8463\n",
      "Epoch 40/75\n",
      "781/781 [==============================] - 186s - loss: 0.7234 - acc: 0.8072 - val_loss: 0.6031 - val_acc: 0.8483\n",
      "Epoch 41/75\n",
      "781/781 [==============================] - 191s - loss: 0.7143 - acc: 0.8089 - val_loss: 0.5890 - val_acc: 0.8540\n",
      "Epoch 42/75\n",
      "781/781 [==============================] - 189s - loss: 0.7058 - acc: 0.8124 - val_loss: 0.5804 - val_acc: 0.8564\n",
      "Epoch 43/75\n",
      "781/781 [==============================] - 192s - loss: 0.6985 - acc: 0.8130 - val_loss: 0.6070 - val_acc: 0.8495\n",
      "Epoch 44/75\n",
      "781/781 [==============================] - 187s - loss: 0.6907 - acc: 0.8144 - val_loss: 0.5830 - val_acc: 0.8543\n",
      "Epoch 45/75\n",
      "781/781 [==============================] - 188s - loss: 0.6878 - acc: 0.8154 - val_loss: 0.5938 - val_acc: 0.8488\n",
      "Epoch 46/75\n",
      "781/781 [==============================] - 196s - loss: 0.6819 - acc: 0.8163 - val_loss: 0.5595 - val_acc: 0.8627\n",
      "Epoch 47/75\n",
      "781/781 [==============================] - 187s - loss: 0.6752 - acc: 0.8186 - val_loss: 0.5718 - val_acc: 0.8563\n",
      "Epoch 48/75\n",
      "781/781 [==============================] - 192s - loss: 0.6763 - acc: 0.8171 - val_loss: 0.5810 - val_acc: 0.8554\n",
      "Epoch 49/75\n",
      "781/781 [==============================] - 189s - loss: 0.6742 - acc: 0.8193 - val_loss: 0.5623 - val_acc: 0.8604\n",
      "Epoch 50/75\n",
      "781/781 [==============================] - 192s - loss: 0.6649 - acc: 0.8225 - val_loss: 0.5460 - val_acc: 0.8654\n",
      "Epoch 51/75\n",
      "781/781 [==============================] - 190s - loss: 0.6586 - acc: 0.8242 - val_loss: 0.5376 - val_acc: 0.8677\n",
      "Epoch 52/75\n",
      "781/781 [==============================] - 187s - loss: 0.6600 - acc: 0.8232 - val_loss: 0.5461 - val_acc: 0.8662\n",
      "Epoch 53/75\n",
      "781/781 [==============================] - 195s - loss: 0.6560 - acc: 0.8242 - val_loss: 0.5562 - val_acc: 0.8594\n",
      "Epoch 54/75\n",
      "781/781 [==============================] - 192s - loss: 0.6541 - acc: 0.8259 - val_loss: 0.5891 - val_acc: 0.8558\n",
      "Epoch 55/75\n",
      "781/781 [==============================] - 194s - loss: 0.6521 - acc: 0.8260 - val_loss: 0.5480 - val_acc: 0.8640\n",
      "Epoch 56/75\n",
      "781/781 [==============================] - 192s - loss: 0.6517 - acc: 0.8261 - val_loss: 0.5439 - val_acc: 0.8640\n",
      "Epoch 57/75\n",
      "781/781 [==============================] - 190s - loss: 0.6416 - acc: 0.8292 - val_loss: 0.5418 - val_acc: 0.8625\n",
      "Epoch 58/75\n",
      "781/781 [==============================] - 194s - loss: 0.6472 - acc: 0.8279 - val_loss: 0.5506 - val_acc: 0.8637\n",
      "Epoch 59/75\n",
      "781/781 [==============================] - 189s - loss: 0.6447 - acc: 0.8275 - val_loss: 0.5894 - val_acc: 0.8491\n",
      "Epoch 60/75\n",
      "781/781 [==============================] - 192s - loss: 0.6437 - acc: 0.8301 - val_loss: 0.5444 - val_acc: 0.8647\n",
      "Epoch 61/75\n",
      "781/781 [==============================] - 192s - loss: 0.6407 - acc: 0.8270 - val_loss: 0.5322 - val_acc: 0.8682\n",
      "Epoch 62/75\n",
      "781/781 [==============================] - 191s - loss: 0.6376 - acc: 0.8314 - val_loss: 0.5222 - val_acc: 0.8735\n",
      "Epoch 63/75\n",
      "781/781 [==============================] - 188s - loss: 0.6374 - acc: 0.8320 - val_loss: 0.5173 - val_acc: 0.8759\n",
      "Epoch 64/75\n",
      "781/781 [==============================] - 194s - loss: 0.6373 - acc: 0.8314 - val_loss: 0.5431 - val_acc: 0.8700\n",
      "Epoch 65/75\n",
      "781/781 [==============================] - 187s - loss: 0.6370 - acc: 0.8319 - val_loss: 0.5262 - val_acc: 0.8721\n",
      "Epoch 66/75\n",
      "781/781 [==============================] - 192s - loss: 0.6331 - acc: 0.8342 - val_loss: 0.5291 - val_acc: 0.8702\n",
      "Epoch 67/75\n",
      "781/781 [==============================] - 190s - loss: 0.6248 - acc: 0.8341 - val_loss: 0.5229 - val_acc: 0.8780\n",
      "Epoch 68/75\n",
      "781/781 [==============================] - 193s - loss: 0.6286 - acc: 0.8338 - val_loss: 0.5436 - val_acc: 0.8696\n",
      "Epoch 69/75\n",
      "781/781 [==============================] - 199s - loss: 0.6277 - acc: 0.8352 - val_loss: 0.5165 - val_acc: 0.8766\n",
      "Epoch 70/75\n",
      "781/781 [==============================] - 185s - loss: 0.6294 - acc: 0.8321 - val_loss: 0.5322 - val_acc: 0.8710\n",
      "Epoch 71/75\n",
      "781/781 [==============================] - 188s - loss: 0.6305 - acc: 0.8343 - val_loss: 0.5260 - val_acc: 0.8725\n",
      "Epoch 72/75\n",
      "781/781 [==============================] - 196s - loss: 0.6196 - acc: 0.8381 - val_loss: 0.5327 - val_acc: 0.8726\n",
      "Epoch 73/75\n",
      "781/781 [==============================] - 188s - loss: 0.6293 - acc: 0.8340 - val_loss: 0.5512 - val_acc: 0.8636\n",
      "Epoch 74/75\n",
      "781/781 [==============================] - 184s - loss: 0.6247 - acc: 0.8365 - val_loss: 0.5415 - val_acc: 0.8679\n",
      "Epoch 75/75\n",
      "781/781 [==============================] - 190s - loss: 0.6198 - acc: 0.8360 - val_loss: 0.5286 - val_acc: 0.8734\n",
      "Epoch 1/25\n",
      "781/781 [==============================] - 191s - loss: 0.5776 - acc: 0.8497 - val_loss: 0.4876 - val_acc: 0.8845\n",
      "Epoch 2/25\n",
      "781/781 [==============================] - 189s - loss: 0.5656 - acc: 0.8536 - val_loss: 0.4928 - val_acc: 0.8838\n",
      "Epoch 3/25\n",
      "781/781 [==============================] - 189s - loss: 0.5512 - acc: 0.8581 - val_loss: 0.4865 - val_acc: 0.8840\n",
      "Epoch 4/25\n",
      "781/781 [==============================] - 184s - loss: 0.5523 - acc: 0.8567 - val_loss: 0.4687 - val_acc: 0.8889\n",
      "Epoch 5/25\n",
      "781/781 [==============================] - 188s - loss: 0.5449 - acc: 0.8571 - val_loss: 0.4683 - val_acc: 0.8886\n",
      "Epoch 6/25\n",
      "781/781 [==============================] - 201s - loss: 0.5448 - acc: 0.8584 - val_loss: 0.4711 - val_acc: 0.8884\n",
      "Epoch 7/25\n",
      "781/781 [==============================] - 193s - loss: 0.5373 - acc: 0.8610 - val_loss: 0.4758 - val_acc: 0.8834\n",
      "Epoch 8/25\n",
      "781/781 [==============================] - 194s - loss: 0.5335 - acc: 0.8618 - val_loss: 0.4683 - val_acc: 0.8865\n",
      "Epoch 9/25\n",
      "781/781 [==============================] - 193s - loss: 0.5274 - acc: 0.8620 - val_loss: 0.4553 - val_acc: 0.8896\n",
      "Epoch 10/25\n",
      "781/781 [==============================] - 193s - loss: 0.5282 - acc: 0.8626 - val_loss: 0.4643 - val_acc: 0.8873\n",
      "Epoch 11/25\n",
      "781/781 [==============================] - 192s - loss: 0.5252 - acc: 0.8617 - val_loss: 0.4524 - val_acc: 0.8906\n",
      "Epoch 12/25\n",
      "781/781 [==============================] - 193s - loss: 0.5238 - acc: 0.8629 - val_loss: 0.4524 - val_acc: 0.8897\n",
      "Epoch 13/25\n",
      "781/781 [==============================] - 200s - loss: 0.5224 - acc: 0.8638 - val_loss: 0.4519 - val_acc: 0.8894\n",
      "Epoch 14/25\n",
      "781/781 [==============================] - 189s - loss: 0.5187 - acc: 0.8627 - val_loss: 0.4614 - val_acc: 0.8859\n",
      "Epoch 15/25\n",
      "781/781 [==============================] - 200s - loss: 0.5122 - acc: 0.8658 - val_loss: 0.4599 - val_acc: 0.8862\n",
      "Epoch 16/25\n",
      "781/781 [==============================] - 193s - loss: 0.5193 - acc: 0.8609 - val_loss: 0.4497 - val_acc: 0.8880\n",
      "Epoch 17/25\n",
      "781/781 [==============================] - 185s - loss: 0.5108 - acc: 0.8653 - val_loss: 0.4426 - val_acc: 0.8917\n",
      "Epoch 18/25\n",
      "781/781 [==============================] - 191s - loss: 0.5064 - acc: 0.8662 - val_loss: 0.4575 - val_acc: 0.8895\n",
      "Epoch 19/25\n",
      "781/781 [==============================] - 195s - loss: 0.5136 - acc: 0.8634 - val_loss: 0.4419 - val_acc: 0.8918\n",
      "Epoch 20/25\n",
      "781/781 [==============================] - 186s - loss: 0.5123 - acc: 0.8641 - val_loss: 0.4626 - val_acc: 0.8872\n",
      "Epoch 21/25\n",
      "781/781 [==============================] - 192s - loss: 0.5059 - acc: 0.8672 - val_loss: 0.4439 - val_acc: 0.8888\n",
      "Epoch 22/25\n",
      "781/781 [==============================] - 192s - loss: 0.5049 - acc: 0.8650 - val_loss: 0.4468 - val_acc: 0.8894\n",
      "Epoch 23/25\n",
      "781/781 [==============================] - 190s - loss: 0.5092 - acc: 0.8636 - val_loss: 0.4373 - val_acc: 0.8898\n",
      "Epoch 24/25\n",
      "781/781 [==============================] - 193s - loss: 0.5043 - acc: 0.8655 - val_loss: 0.4497 - val_acc: 0.8862\n",
      "Epoch 25/25\n",
      "781/781 [==============================] - 194s - loss: 0.5000 - acc: 0.8664 - val_loss: 0.4567 - val_acc: 0.8865\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 200s - loss: 0.4773 - acc: 0.8759 - val_loss: 0.4351 - val_acc: 0.8931\n",
      "Epoch 2/25\n",
      "781/781 [==============================] - 193s - loss: 0.4753 - acc: 0.8748 - val_loss: 0.4182 - val_acc: 0.8970\n",
      "Epoch 3/25\n",
      "781/781 [==============================] - 195s - loss: 0.4691 - acc: 0.8775 - val_loss: 0.4184 - val_acc: 0.8979\n",
      "Epoch 4/25\n",
      "781/781 [==============================] - 193s - loss: 0.4705 - acc: 0.8754 - val_loss: 0.4115 - val_acc: 0.8979\n",
      "Epoch 5/25\n",
      "781/781 [==============================] - 187s - loss: 0.4618 - acc: 0.8770 - val_loss: 0.4060 - val_acc: 0.9018\n",
      "Epoch 6/25\n",
      "781/781 [==============================] - 188s - loss: 0.4613 - acc: 0.8779 - val_loss: 0.4126 - val_acc: 0.8992\n",
      "Epoch 7/25\n",
      "781/781 [==============================] - 190s - loss: 0.4598 - acc: 0.8794 - val_loss: 0.4093 - val_acc: 0.8995\n",
      "Epoch 8/25\n",
      "781/781 [==============================] - 196s - loss: 0.4547 - acc: 0.8794 - val_loss: 0.4124 - val_acc: 0.8971\n",
      "Epoch 9/25\n",
      "781/781 [==============================] - 193s - loss: 0.4607 - acc: 0.8782 - val_loss: 0.4171 - val_acc: 0.8976\n",
      "Epoch 10/25\n",
      "781/781 [==============================] - 190s - loss: 0.4555 - acc: 0.8795 - val_loss: 0.3986 - val_acc: 0.9020\n",
      "Epoch 11/25\n",
      "781/781 [==============================] - 194s - loss: 0.4519 - acc: 0.8795 - val_loss: 0.4066 - val_acc: 0.9005\n",
      "Epoch 12/25\n",
      "781/781 [==============================] - 201s - loss: 0.4552 - acc: 0.8787 - val_loss: 0.4029 - val_acc: 0.8993\n",
      "Epoch 13/25\n",
      "781/781 [==============================] - 195s - loss: 0.4508 - acc: 0.8802 - val_loss: 0.4064 - val_acc: 0.9003\n",
      "Epoch 14/25\n",
      "781/781 [==============================] - 195s - loss: 0.4441 - acc: 0.8822 - val_loss: 0.4033 - val_acc: 0.9003\n",
      "Epoch 15/25\n",
      "781/781 [==============================] - 198s - loss: 0.4485 - acc: 0.8797 - val_loss: 0.4048 - val_acc: 0.8991\n",
      "Epoch 16/25\n",
      "781/781 [==============================] - 198s - loss: 0.4448 - acc: 0.8808 - val_loss: 0.4034 - val_acc: 0.9010\n",
      "Epoch 17/25\n",
      "781/781 [==============================] - 191s - loss: 0.4457 - acc: 0.8804 - val_loss: 0.4056 - val_acc: 0.8996\n",
      "Epoch 18/25\n",
      "781/781 [==============================] - 197s - loss: 0.4426 - acc: 0.8832 - val_loss: 0.4000 - val_acc: 0.9025\n",
      "Epoch 19/25\n",
      "781/781 [==============================] - 197s - loss: 0.4424 - acc: 0.8802 - val_loss: 0.3986 - val_acc: 0.9017\n",
      "Epoch 20/25\n",
      "781/781 [==============================] - 195s - loss: 0.4392 - acc: 0.8824 - val_loss: 0.4015 - val_acc: 0.9001\n",
      "Epoch 21/25\n",
      "781/781 [==============================] - 196s - loss: 0.4414 - acc: 0.8826 - val_loss: 0.4025 - val_acc: 0.9026\n",
      "Epoch 22/25\n",
      "781/781 [==============================] - 194s - loss: 0.4413 - acc: 0.8805 - val_loss: 0.4004 - val_acc: 0.9011\n",
      "Epoch 23/25\n",
      "781/781 [==============================] - 196s - loss: 0.4409 - acc: 0.8814 - val_loss: 0.4040 - val_acc: 0.9000\n",
      "Epoch 24/25\n",
      "781/781 [==============================] - 198s - loss: 0.4382 - acc: 0.8825 - val_loss: 0.3873 - val_acc: 0.9046\n",
      "Epoch 25/25\n",
      "781/781 [==============================] - 193s - loss: 0.4334 - acc: 0.8835 - val_loss: 0.3935 - val_acc: 0.9020\n",
      " 9984/10000 [============================>.] - ETA: 0s\n",
      "Test result: 90.200 loss: 0.393\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "from keras import regularizers, optimizers\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#z-score\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    "\n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)\n",
    "\n",
    "baseMapNum = 32\n",
    "weight_decay = 1e-4\n",
    "model = Sequential()\n",
    "model.add(Conv2D(baseMapNum, (3,3), \n",
    "                 padding='same', \n",
    "                 kernel_regularizer=regularizers.l2(weight_decay), \n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(2*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(2*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(4*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(4*baseMapNum, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False\n",
    "    )\n",
    "datagen.fit(x_train)\n",
    "\n",
    "#training\n",
    "batch_size = 64\n",
    "epochs=25\n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "        optimizer=opt_rms,\n",
    "        metrics=['accuracy'])\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=3*epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test,y_test))\n",
    "model.save_weights('cifar10_normal_rms_ep75.h5')\n",
    "\n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.0005,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "        optimizer=opt_rms,\n",
    "        metrics=['accuracy'])\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test,y_test))\n",
    "model.save_weights('cifar10_normal_rms_ep100.h5')\n",
    "\n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.0003,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "        optimizer=opt_rms,\n",
    "        metrics=['accuracy'])\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test,y_test))\n",
    "model.save_weights('cifar10_normal_rms_ep125.h5')\n",
    "\n",
    "#testing - no kaggle eval\n",
    "scores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
    "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_71 (Conv2D)           (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_70 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_72 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_71 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_73 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_72 (Batc (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_74 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_73 (Batc (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_75 (Conv2D)           (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_74 (Batc (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_76 (Conv2D)           (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_75 (Batc (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 1,189,962\n",
      "Trainable params: 1,188,170\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "781/781 [==============================] - 496s - loss: 2.3194 - acc: 0.3595 - val_loss: 1.3669 - val_acc: 0.5157\n",
      "Epoch 2/25\n",
      "781/781 [==============================] - 484s - loss: 1.6262 - acc: 0.4802 - val_loss: 1.1519 - val_acc: 0.5952\n",
      "Epoch 3/25\n",
      "781/781 [==============================] - 494s - loss: 1.3680 - acc: 0.5548 - val_loss: 1.0129 - val_acc: 0.6554\n",
      "Epoch 4/25\n",
      "781/781 [==============================] - 491s - loss: 1.2189 - acc: 0.6028 - val_loss: 0.9148 - val_acc: 0.6933\n",
      "Epoch 5/25\n",
      "781/781 [==============================] - 490s - loss: 1.1135 - acc: 0.6352 - val_loss: 0.8487 - val_acc: 0.7159\n",
      "Epoch 6/25\n",
      "781/781 [==============================] - 489s - loss: 1.0245 - acc: 0.6622 - val_loss: 0.7597 - val_acc: 0.7452\n",
      "Epoch 7/25\n",
      "781/781 [==============================] - 491s - loss: 0.9625 - acc: 0.6807 - val_loss: 0.7103 - val_acc: 0.7581\n",
      "Epoch 8/25\n",
      "781/781 [==============================] - 495s - loss: 0.9112 - acc: 0.6974 - val_loss: 0.7284 - val_acc: 0.7672\n",
      "Epoch 9/25\n",
      "781/781 [==============================] - 493s - loss: 0.8607 - acc: 0.7126 - val_loss: 0.6934 - val_acc: 0.7723\n",
      "Epoch 10/25\n",
      "781/781 [==============================] - 494s - loss: 0.8118 - acc: 0.7282 - val_loss: 0.6368 - val_acc: 0.7894\n",
      "Epoch 11/25\n",
      "781/781 [==============================] - 488s - loss: 0.7837 - acc: 0.7355 - val_loss: 0.6261 - val_acc: 0.7929\n",
      "Epoch 12/25\n",
      "781/781 [==============================] - 480s - loss: 0.7569 - acc: 0.7446 - val_loss: 0.5789 - val_acc: 0.8078\n",
      "Epoch 13/25\n",
      "781/781 [==============================] - 495s - loss: 0.7267 - acc: 0.7559 - val_loss: 0.5709 - val_acc: 0.8036\n",
      "Epoch 14/25\n",
      "781/781 [==============================] - 493s - loss: 0.6965 - acc: 0.7651 - val_loss: 0.5464 - val_acc: 0.8109\n",
      "Epoch 15/25\n",
      "781/781 [==============================] - 488s - loss: 0.6845 - acc: 0.7700 - val_loss: 0.5318 - val_acc: 0.8202\n",
      "Epoch 16/25\n",
      "781/781 [==============================] - 486s - loss: 0.6561 - acc: 0.7773 - val_loss: 0.5225 - val_acc: 0.8243\n",
      "Epoch 17/25\n",
      "781/781 [==============================] - 487s - loss: 0.6395 - acc: 0.7843 - val_loss: 0.5148 - val_acc: 0.8264\n",
      "Epoch 18/25\n",
      "781/781 [==============================] - 490s - loss: 0.6210 - acc: 0.7877 - val_loss: 0.5034 - val_acc: 0.8304\n",
      "Epoch 19/25\n",
      "781/781 [==============================] - 484s - loss: 0.6030 - acc: 0.7954 - val_loss: 0.4678 - val_acc: 0.8429\n",
      "Epoch 20/25\n",
      "781/781 [==============================] - 497s - loss: 0.5894 - acc: 0.7996 - val_loss: 0.4715 - val_acc: 0.8457\n",
      "Epoch 21/25\n",
      "781/781 [==============================] - 490s - loss: 0.5697 - acc: 0.8049 - val_loss: 0.4861 - val_acc: 0.8394\n",
      "Epoch 22/25\n",
      "781/781 [==============================] - 491s - loss: 0.5668 - acc: 0.8080 - val_loss: 0.4645 - val_acc: 0.8444\n",
      "Epoch 23/25\n",
      "781/781 [==============================] - 487s - loss: 0.5497 - acc: 0.8114 - val_loss: 0.4450 - val_acc: 0.8493\n",
      "Epoch 24/25\n",
      "781/781 [==============================] - 489s - loss: 0.5400 - acc: 0.8160 - val_loss: 0.4528 - val_acc: 0.8513\n",
      "Epoch 25/25\n",
      "781/781 [==============================] - 494s - loss: 0.5250 - acc: 0.8191 - val_loss: 0.4220 - val_acc: 0.8589\n",
      "10000/10000 [==============================] - 37s    \n",
      "\n",
      "Test result: 85.890 loss: 0.422\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "from keras import regularizers, optimizers\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#z-score\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    "\n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)\n",
    "\n",
    "baseMapNum = 64\n",
    "weight_decay = 1e-7\n",
    "model = Sequential()\n",
    "model.add(Conv2D(baseMapNum, (3,3), \n",
    "                 padding='same', \n",
    "                 activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay), \n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(baseMapNum, (3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(2*baseMapNum, (3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(2*baseMapNum, (3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(4*baseMapNum, (3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(4*baseMapNum, (3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False\n",
    "    )\n",
    "datagen.fit(x_train)\n",
    "\n",
    "#training\n",
    "batch_size = 64\n",
    "epochs=25\n",
    "\n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.0001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "        optimizer=opt_rms,\n",
    "        metrics=['accuracy'])\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test,y_test))\n",
    "model.save_weights('cifar10_normal_rms_ep125.h5')\n",
    "\n",
    "#testing - no kaggle eval\n",
    "scores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
    "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
