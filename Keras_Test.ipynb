{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.3574 - acc: 0.0990     \n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.3111 - acc: 0.1230     \n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2946 - acc: 0.1090     \n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2820 - acc: 0.1150     \n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2678 - acc: 0.1380     \n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2606 - acc: 0.1480     \n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2494 - acc: 0.1620     \n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2445 - acc: 0.1610     \n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2330 - acc: 0.1740     \n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2229 - acc: 0.1830     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffafd3a2c18>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "# For a single-input model with 10 classes (categorical classification):\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(10, size=(1000, 1))\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "one_hot_labels = keras.utils.to_categorical(labels, num_classes=10)\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, one_hot_labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 20)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random((1000, 20)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.7068 - acc: 0.4780     \n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6969 - acc: 0.4970     \n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6930 - acc: 0.5210     \n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6886 - acc: 0.5380     \n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6858 - acc: 0.5370     \n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6829 - acc: 0.5710     \n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6806 - acc: 0.5630     \n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6760 - acc: 0.5780     \n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6744 - acc: 0.5990     \n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.6675 - acc: 0.5910     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffaf8033e80>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For a single-input model with 2 classes (binary classification):\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(2, size=(1000, 1))\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.3255 - acc: 0.1160     \n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2993 - acc: 0.1230     \n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2854 - acc: 0.1310     \n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2732 - acc: 0.1550     \n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2622 - acc: 0.1520     \n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2510 - acc: 0.1480     \n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2403 - acc: 0.1620     \n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2260 - acc: 0.1780     \n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2131 - acc: 0.1870     \n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2036 - acc: 0.1980     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffaf0547e10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For a single-input model with 10 classes (categorical classification):\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))\n",
    "labels = np.random.randint(10, size=(1000, 1))\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "one_hot_labels = keras.utils.to_categorical(labels, num_classes=10)\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, one_hot_labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.4244 - acc: 0.0900     \n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3669 - acc: 0.1090     \n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3454 - acc: 0.1040     \n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3267 - acc: 0.0870     \n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3247 - acc: 0.1030     \n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3294 - acc: 0.0920     \n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3182 - acc: 0.1050     \n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3139 - acc: 0.1000     \n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3094 - acc: 0.1160     \n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3122 - acc: 0.1080     \n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3081 - acc: 0.0990     \n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3050 - acc: 0.1120     \n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3054 - acc: 0.1180     \n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3089 - acc: 0.1030     \n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3023 - acc: 0.1020     \n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3029 - acc: 0.1090     \n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3029 - acc: 0.1150     \n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3072 - acc: 0.1020     \n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.3000 - acc: 0.1080     \n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s - loss: 2.2989 - acc: 0.1220     \n",
      "100/100 [==============================] - 0s\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Generate dummy data\n",
    "import numpy as np\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape:\n",
    "# here, 20-dimensional vectors.\n",
    "model.add(Dense(64, activation='relu', input_dim=20))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.3030779361724854, 0.11999999731779099]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.7033 - acc: 0.5170     \n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6986 - acc: 0.5020     \n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6978 - acc: 0.5180     \n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6926 - acc: 0.5170     \n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6947 - acc: 0.5170     \n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6882 - acc: 0.5350     \n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6959 - acc: 0.5000     \n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6921 - acc: 0.5200     \n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6951 - acc: 0.5100     \n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6929 - acc: 0.5280     \n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6977 - acc: 0.5040     \n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6895 - acc: 0.5350     \n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6917 - acc: 0.5240     \n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6854 - acc: 0.5520     \n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6915 - acc: 0.5280     \n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6867 - acc: 0.5490     \n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.6940 - acc: 0.500 - 0s - loss: 0.6896 - acc: 0.5380     \n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6889 - acc: 0.5370     \n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6875 - acc: 0.5520     \n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s - loss: 0.6806 - acc: 0.5770     \n",
      "100/100 [==============================] - 0s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# Generate dummy data\n",
    "x_train = np.random.random((1000, 20))\n",
    "y_train = np.random.randint(2, size=(1000, 1))\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = np.random.randint(2, size=(100, 1))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=20, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 2s - loss: 2.3151     \n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 3s - loss: 2.3396     \n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 2s - loss: 2.2961     \n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 2s - loss: 2.2517     \n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 3s - loss: 2.3073     \n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 3s - loss: 2.2837     \n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 2s - loss: 2.2810     \n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 2s - loss: 2.2813     \n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 2s - loss: 2.2721     \n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 1s - loss: 2.2918     \n",
      "20/20 [==============================] - 0s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Generate dummy data\n",
    "x_train = np.random.random((100, 100, 100, 3))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n",
    "x_test = np.random.random((20, 100, 100, 3))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(20, 1)), num_classes=10)\n",
    "\n",
    "model = Sequential()\n",
    "# input: 100x100 images with 3 channels -> (100, 100, 3) tensors.\n",
    "# this applies 32 convolution filters of size 3x3 each.\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=10)\n",
    "score = model.evaluate(x_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11476992/11490434 [============================>.] - ETA: 0s60000 train samples\n",
      "10000 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 9s - loss: 0.2449 - acc: 0.9264 - val_loss: 0.0976 - val_acc: 0.9697\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 8s - loss: 0.1023 - acc: 0.9683 - val_loss: 0.0770 - val_acc: 0.9772\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 8s - loss: 0.0743 - acc: 0.9774 - val_loss: 0.0774 - val_acc: 0.9775\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 9s - loss: 0.0589 - acc: 0.9817 - val_loss: 0.0748 - val_acc: 0.9796\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 8s - loss: 0.0521 - acc: 0.9845 - val_loss: 0.0768 - val_acc: 0.9799\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 6s - loss: 0.0450 - acc: 0.9868 - val_loss: 0.0728 - val_acc: 0.9820\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 7s - loss: 0.0380 - acc: 0.9885 - val_loss: 0.0811 - val_acc: 0.9825\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 9s - loss: 0.0336 - acc: 0.9901 - val_loss: 0.0785 - val_acc: 0.9829\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 10s - loss: 0.0311 - acc: 0.9909 - val_loss: 0.0877 - val_acc: 0.9826\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 8s - loss: 0.0288 - acc: 0.9917 - val_loss: 0.0998 - val_acc: 0.9797\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 9s - loss: 0.0272 - acc: 0.9918 - val_loss: 0.0924 - val_acc: 0.9835\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 9s - loss: 0.0241 - acc: 0.9930 - val_loss: 0.1000 - val_acc: 0.9814\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 8s - loss: 0.0233 - acc: 0.9933 - val_loss: 0.0941 - val_acc: 0.9828\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 6s - loss: 0.0220 - acc: 0.9941 - val_loss: 0.1006 - val_acc: 0.9830\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 6s - loss: 0.0224 - acc: 0.9941 - val_loss: 0.0989 - val_acc: 0.9831\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 7s - loss: 0.0206 - acc: 0.9947 - val_loss: 0.1053 - val_acc: 0.9833\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 5s - loss: 0.0205 - acc: 0.9945 - val_loss: 0.1047 - val_acc: 0.9824\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 8s - loss: 0.0193 - acc: 0.9952 - val_loss: 0.0968 - val_acc: 0.9854\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 8s - loss: 0.0175 - acc: 0.9950 - val_loss: 0.1072 - val_acc: 0.9829\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 8s - loss: 0.0186 - acc: 0.9951 - val_loss: 0.1129 - val_acc: 0.9837\n",
      "Test loss: 0.11291309957\n",
      "Test accuracy: 0.9837\n"
     ]
    }
   ],
   "source": [
    "'''Trains a simple deep NN on the MNIST dataset.\n",
    "Gets to 98.40% test accuracy after 20 epochs\n",
    "(there is *a lot* of margin for parameter tuning).\n",
    "2 seconds per epoch on a K520 GPU.\n",
    "'''\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 43s - loss: 0.3215 - acc: 0.9012 - val_loss: 0.0735 - val_acc: 0.9776\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 44s - loss: 0.1105 - acc: 0.9671 - val_loss: 0.0526 - val_acc: 0.9823\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 43s - loss: 0.0853 - acc: 0.9745 - val_loss: 0.0446 - val_acc: 0.9845\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 42s - loss: 0.0712 - acc: 0.9784 - val_loss: 0.0431 - val_acc: 0.9866\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 42s - loss: 0.0621 - acc: 0.9814 - val_loss: 0.0346 - val_acc: 0.9885\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 43s - loss: 0.0566 - acc: 0.9827 - val_loss: 0.0340 - val_acc: 0.9891\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 41s - loss: 0.0511 - acc: 0.9846 - val_loss: 0.0320 - val_acc: 0.9898\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 41s - loss: 0.0487 - acc: 0.9859 - val_loss: 0.0310 - val_acc: 0.9893\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 45s - loss: 0.0446 - acc: 0.9867 - val_loss: 0.0295 - val_acc: 0.9906\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 52s - loss: 0.0431 - acc: 0.9872 - val_loss: 0.0296 - val_acc: 0.9902\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 41s - loss: 0.0398 - acc: 0.9884 - val_loss: 0.0297 - val_acc: 0.9895\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 41s - loss: 0.0375 - acc: 0.9884 - val_loss: 0.0288 - val_acc: 0.9904\n",
      "Test loss: 0.0287521056042\n",
      "Test accuracy: 0.9904\n"
     ]
    }
   ],
   "source": [
    "'''Trains a simple convnet on the MNIST dataset.\n",
    "Gets to 99.25% test accuracy after 12 epochs\n",
    "(there is still a lot of margin for parameter tuning).\n",
    "16 seconds per epoch on a GRID K520 GPU.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAC1CAYAAABPoAT2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noCsUCoWToAK6QqFQOAn/D21xaqaNxavUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ffa54bd4d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(2,10)\n",
    "fig.figsize = (30,6)\n",
    "\n",
    "for i,ax in zip(range(20),axes.ravel()):\n",
    "    ax.imshow(x_train[i].reshape(28,28))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 125s - loss: 0.4118 - acc: 0.8012 - val_loss: 0.3007 - val_acc: 0.8711\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 124s - loss: 0.2438 - acc: 0.9013 - val_loss: 0.2688 - val_acc: 0.8881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffa54c6d3c8>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''This example demonstrates the use of Convolution1D for text classification.\n",
    "Gets to 0.89 test accuracy after 2 epochs.\n",
    "90s/epoch on Intel i5 2.4Ghz CPU.\n",
    "10s/epoch on Tesla K40 GPU.\n",
    "'''\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 400, 50)           250000    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 400, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 398, 250)          37750     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 350,751\n",
      "Trainable params: 350,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "Using real-time data augmentation.\n",
      "Epoch 1/20\n",
      "390/390 [==============================] - 182s - loss: 1.9686 - acc: 0.3336 - val_loss: 3.8690 - val_acc: 0.2233\n",
      "Epoch 2/20\n",
      "390/390 [==============================] - 176s - loss: 1.5750 - acc: 0.4390 - val_loss: 1.2900 - val_acc: 0.5444\n",
      "Epoch 3/20\n",
      "390/390 [==============================] - 168s - loss: 1.4464 - acc: 0.4877 - val_loss: 1.2968 - val_acc: 0.5574\n",
      "Epoch 4/20\n",
      "390/390 [==============================] - 181s - loss: 1.3755 - acc: 0.5122 - val_loss: 1.1815 - val_acc: 0.5890\n",
      "Epoch 5/20\n",
      "390/390 [==============================] - 549s - loss: 1.3035 - acc: 0.5385 - val_loss: 1.1246 - val_acc: 0.6138\n",
      "Epoch 6/20\n",
      "390/390 [==============================] - 196s - loss: 1.2327 - acc: 0.5662 - val_loss: 1.1104 - val_acc: 0.6306\n",
      "Epoch 7/20\n",
      "390/390 [==============================] - 207s - loss: 1.1915 - acc: 0.5846 - val_loss: 1.0074 - val_acc: 0.6571\n",
      "Epoch 8/20\n",
      "390/390 [==============================] - 234s - loss: 1.1513 - acc: 0.5947 - val_loss: 1.0725 - val_acc: 0.6346\n",
      "Epoch 9/20\n",
      "390/390 [==============================] - 228s - loss: 1.1125 - acc: 0.6117 - val_loss: 0.9761 - val_acc: 0.6679\n",
      "Epoch 10/20\n",
      "390/390 [==============================] - 195s - loss: 1.0803 - acc: 0.6224 - val_loss: 0.9625 - val_acc: 0.6714\n",
      "Epoch 11/20\n",
      "390/390 [==============================] - 212s - loss: 1.0456 - acc: 0.6370 - val_loss: 0.9325 - val_acc: 0.6804\n",
      "Epoch 12/20\n",
      "390/390 [==============================] - 200s - loss: 1.0247 - acc: 0.6452 - val_loss: 0.8943 - val_acc: 0.6913\n",
      "Epoch 13/20\n",
      "390/390 [==============================] - 208s - loss: 0.9972 - acc: 0.6555 - val_loss: 0.8822 - val_acc: 0.6992\n",
      "Epoch 14/20\n",
      "390/390 [==============================] - 195s - loss: 0.9770 - acc: 0.6604 - val_loss: 0.8832 - val_acc: 0.7011\n",
      "Epoch 15/20\n",
      "390/390 [==============================] - 196s - loss: 0.9606 - acc: 0.6685 - val_loss: 0.8883 - val_acc: 0.6978\n",
      "Epoch 16/20\n",
      "390/390 [==============================] - 198s - loss: 0.9384 - acc: 0.6760 - val_loss: 0.8094 - val_acc: 0.7266\n",
      "Epoch 17/20\n",
      "390/390 [==============================] - 209s - loss: 0.9242 - acc: 0.6827 - val_loss: 0.8741 - val_acc: 0.7057\n",
      "Epoch 18/20\n",
      "390/390 [==============================] - 207s - loss: 0.9094 - acc: 0.6876 - val_loss: 0.8172 - val_acc: 0.7222\n",
      "Epoch 19/20\n",
      "390/390 [==============================] - 215s - loss: 0.8967 - acc: 0.6917 - val_loss: 0.7912 - val_acc: 0.7329\n",
      "Epoch 20/20\n",
      "390/390 [==============================] - 196s - loss: 0.8821 - acc: 0.6957 - val_loss: 0.7910 - val_acc: 0.7309\n",
      "Saved trained model at /root/ml/saved_models/keras_cifar10_trained_model.h5 \n",
      " 9984/10000 [============================>.] - ETA: 0sTest loss: 0.791049369621\n",
      "Test accuracy: 0.7309\n"
     ]
    }
   ],
   "source": [
    "'''Train a simple deep CNN on the CIFAR10 small images dataset.\n",
    "GPU run command with Theano backend (with TensorFlow, the GPU is automatically used):\n",
    "    THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatx=float32 python cifar10_cnn.py\n",
    "It gets down to 0.65 test logloss in 25 epochs, and down to 0.55 after 50 epochs.\n",
    "(it's still underfitting at that point, though).\n",
    "'''\n",
    "\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers\n",
    "\n",
    "import os\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "weight_decay = 1e-4\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay),\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3),padding='same', activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
